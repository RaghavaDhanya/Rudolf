{\chapter{Code snippets}\label{ch:scope}}
\epigraph{\textit{\normalsize “By far the greatest danger of Artificial Intelligence is that people conclude too early that they understand it.”}}{\textit{ \normalsize Eliezer Yudkowsky,\\ Machine Intelligence Research Institute}}


\section{GAN Training} % (fold)
\label{sec:gan_training}
The internal architectures of all four GANs are similarly designed. Here we take DCGAN to showcase the code.

\par\bigskip 
The main code consists of a class (DCGAN) which contains the following six functions:
\par\bigskip

\begin{enumerate}
    \item Initialization: Calls build\_generator and build\_discriminator and makes a combined model
    \item Build\_Generator: Creates a generator model
    \item Build\_Discriminator: Creates a discriminator model
    \item Train: Takes the input images and starts training, prints training progress with metrics
    \item Save\_Imgs: Saves a grid of generated images at specific epochs
    \item Save\_models: Saves the current model to disk
\end{enumerate}

\subsection{Initialization} % (fold)
\label{subsec:initialization}
\begin{lstlisting}[basicstyle=\scriptsize,language=Python]
def __init__(self):
    # Input shape
    self.img_rows = 64
    self.img_cols = 64
    self.channels = 3
    self.img_shape = (self.img_rows, self.img_cols, self.channels)
    self.latent_dim = 100

    optimizer = Adam(0.0002, 0.5)

    # Build and compile the discriminator
    self.discriminator = self.build_discriminator()
    self.discriminator.compile(loss='binary_crossentropy',
        optimizer=optimizer,
        metrics=['accuracy'])

    # Build the generator
    self.generator = self.build_generator()

    # The generator takes noise as input and generates imgs
    z = Input(shape=(100,))
    img = self.generator(z)

    # For the combined model only train the generator
    self.discriminator.trainable = False

    # The discriminator takes generated images as 
    # input and determines validity
    valid = self.discriminator(img)

    # The combined model  (stacked generator and discriminator)
    self.combined = Model(z, valid)
    self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)
\end{lstlisting}
\par\bigskip

% subsection initialization (end)

\subsection{Build Generator} % (fold)
\label{subsec:building_generator}
\begin{lstlisting}[basicstyle=\scriptsize,language=Python]
def build_generator(self):
    """
    Build generator which takes noise (a tensor of size 100) as input,
    and produces an RGB image of size (64 x 64) .
    """

    # Create a model in which one can add layers sequentially
    model = Sequential()

    # Add a densely connected layer to the model, 
    # activation function of ReLu
    model.add(Dense(128 * 8 * 8, activation="relu", 
        input_shape=(self.latent_dim,)))
    model.add(Reshape((8, 8, 128)))

    # DeConv layer one starts
    model.add(BatchNormalization(momentum=0.8))
    model.add(UpSampling2D())
    model.add(Conv2D(128, kernel_size=3, padding="same"))
    model.add(Activation("relu"))

    # DeConv layer two starts
    model.add(BatchNormalization(momentum=0.8))
    model.add(UpSampling2D())
    model.add(Conv2D(64, kernel_size=3, padding="same"))
    model.add(Activation("relu"))

    # DeConv layer three starts
    model.add(BatchNormalization(momentum=0.8))
    model.add(UpSampling2D())
    model.add(Conv2D(32, kernel_size=3, padding="same"))
    model.add(Activation("relu"))

    # Final output layer
    model.add(BatchNormalization(momentum=0.8))
    model.add(Conv2D(self.channels, kernel_size=3, padding="same"))
    model.add(Activation("tanh"))

    # Print model summary
    model.summary()

    # Use functional API to make model
    noise = Input(shape=(self.latent_dim,))
    img = model(noise)

    # Return functional model
    return Model(noise, img)

\end{lstlisting}
% subsection building_generator (end)
\par\bigskip

\subsection{Build Discriminator} % (fold)
\label{subsec:building_discriminator}
\begin{lstlisting}[basicstyle=\scriptsize,language=Python]
def build_discriminator(self):
    """
    Discriminator takes real/generated images and outputs its prediction.
    """

    # Define input shape for our network
    img = Input(shape=self.img_shape)
    
    # First ConvLayer outputs a 56x56x256 matrix
    x = Conv2D(filters=256, kernel_size=9, strides=1, 
        padding='valid', name='conv1')(img)
    x = LeakyReLU()(x)
    x = BatchNormalization(momentum=0.8)(x)

    # Capsule architecture starts

    # First layer: PrimaryCaps
    x = Conv2D(filters=8 * 32, kernel_size=9, strides=2, 
        padding='valid', name='primarycap_conv2')(x)

    # Primary capsule has collections of activations which denote orientation
    # while intensity of the vector which denotes the presence of the digit)
    x = Reshape(target_shape=[-1, 8], name='primarycap_reshape')(x)

    # Output a number between 0 and 1 for each capsule 
    # where the length of the input decides the amount
    x = Lambda(squash, name='primarycap_squash')(x)
    x = BatchNormalization(momentum=0.8)(x)

    # Second layer: DigitCaps
    # This is a modified form of the standard CapsNet DigitCaps architecture 
    # where we have replaced the multiple capsules with a single capsule of 
    # densely connected neural network.
    x = Flatten()(x)

    # Dynamic Routing
    # uhat = prediction vector, u * w
    # w = weight matrix but will act as a dense layer
    # u = output from a previous layer
    uhat = Dense(160, kernel_initializer='he_normal', 
        bias_initializer='zeros', name='uhat_digitcaps')(x)

    # softmax will make sure that each weight c_ij is a non-negative number 
    # and their sum equals to one
    c = Activation('softmax', name='softmax_digitcaps1')(uhat) 

    # s_j (output of the current capsule level) = uhat * c
    c = Dense(160)(c) # compute s_j
    x = Multiply()([uhat, c])

    # Squashing the capsule outputs creates severe blurry artifacts, 
    # thus we replace it with Leaky ReLu.
    s_j = LeakyReLU()(x)


    c = Activation('softmax', name='softmax_digitcaps2')(s_j) 
    c = Dense(160)(c)
    x = Multiply()([uhat, c])
    s_j = LeakyReLU()(x)

    c = Activation('softmax', name='softmax_digitcaps3')(s_j) 
    c = Dense(160)(c)
    x = Multiply()([uhat, c])
    s_j = LeakyReLU()(x)

    # Final dense layer output a binary classification
    pred = Dense(1, activation='sigmoid')(s_j)
    return Model(img, pred)
\end{lstlisting}
% subsection building_discriminator (end)
\par\bigskip

\subsection{Train} % (fold)
\label{subsec:train}
\begin{lstlisting}[basicstyle=\scriptsize,language=Python]
def train(self, epochs, batch_size=128, save_interval=50):
    
    half_batch = int(batch_size / 2)

    for epoch in range(epochs):
        # ---------------------
        #  Train Discriminator
        # ---------------------
        cnt=0
        train_datagen = ImageDataGenerator(rescale=1./255)
        train_generator = train_datagen.flow_from_directory('data',
            target_size=(64, 64),batch_size=half_batch,class_mode=None)
        for x in train_generator:
            # Sample noise and generate a half batch of new images
            noise = np.random.normal(0, 1, (half_batch, 100))
            gen_imgs = self.generator.predict(noise)

            # Train the discriminator 
            # (real classified as ones and generated as zeros)
            d_loss_real = self.discriminator.train_on_batch
                (x, np.ones((half_batch, 1)))
            d_loss_fake = self.discriminator.train_on_batch
                (gen_imgs, np.zeros((half_batch, 1)))
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

            # ---------------------
            #  Train Generator
            # ---------------------

            # Sample generator input
            noise = np.random.normal(0, 1, (batch_size, 100))

            # Train the generator 
            # (wants discriminator to mistake images as real)
            g_loss = self.combined.train_on_batch
                (noise, np.ones((batch_size, 1)))

            # Plot the progress
            if(cnt%save_interval==0):
              print ("%d [D loss: %f, acc.: %.2f%%] [G loss: %f]" 
                % (epoch, d_loss[0], 100*d_loss[1], g_loss))
            
            # If at save interval => save generated image samples
            if(cnt%save_interval==0):
              self.save_imgs(cnt)
            cnt+=1
\end{lstlisting}
% subsection train (end)
\par\bigskip

\subsection{Save Images} % (fold)
\label{subsec:save_imgs}
\begin{lstlisting}[basicstyle=\scriptsize,language=Python]
def save_imgs(self, epoch):
    r, c = 5, 5
    noise = np.random.normal(0, 1, (r * c, 100))
    gen_imgs = self.generator.predict(noise)

    # Rescale images 0 - 1
    gen_imgs = 0.5 * gen_imgs + 0.5

    fig, axs = plt.subplots(r, c)
    #fig.suptitle("DCGAN: Generated digits", fontsize=12)
    cnt = 0
    for i in range(r):
        for j in range(c):
            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')
            axs[i,j].axis('off')
            cnt += 1
    fig.savefig("images/images_%d.png" % epoch)
    plt.close()
\end{lstlisting}
% subsection save_imgs (end)
\par\bigskip

\subsection{Save Models} % (fold)
\label{sub:saving_models}
\begin{lstlisting}[basicstyle=\scriptsize,language=Python]
def save_model(self):

    def save(model, model_name):
        model_path = "saved_model/%s.json" % model_name
        weights_path = "saved_model/%s_weights.hdf5" % model_name
        options = {"file_arch": model_path,
                    "file_weight": weights_path}
        json_string = model.to_json()
        open(options['file_arch'], 'w').write(json_string)
        model.save_weights(options['file_weight'])

    save(self.generator, "generator")
    save(self.discriminator, "discriminator")
    save(self.combined, "adversarial")
\end{lstlisting}
% subsection saving_models (end)
\par\bigskip

% section gan_training (end)

\section{Demonstration} % (fold)
\label{sec:code_demonstration}

\subsection{Converting Models} % (fold)
\label{sub:converting_models}
This code converts the Keras H5 model to the TensorFlow protocol buffers.
\begin{lstlisting}[basicstyle=\scriptsize,language=Python]
def convert_to_pb(weight_file,json_file,input_fld='',output_fld=''):

    import os
    import os.path as osp
    from tensorflow.python.framework import graph_util
    from tensorflow.python.framework import graph_io
    from keras.models import model_from_json
    from keras import backend as K
    import tensorflow as tf

    # weight_file is a .hdf5 keras model file
    output_node_names_of_input_network = ["pred0"] 
    output_node_names_of_final_network = 'output_node'

    # change filename to a .pb tensorflow file
    output_graph_name = json_file[:-4]+'pb'
    
    weight_file_path = osp.join(input_fld, weight_file)
    json_file_path=osp.join(input_fld,json_file)
    js_file = open(json_file_path, 'r')
    model_js= js_file.read()
    js_file.close()
    net_model = model_from_json(model_js)
    # load weights into new model
    net_model.load_weights(weight_file_path)
    

    num_output = len(output_node_names_of_input_network)
    pred = [None]*num_output
    pred_node_names = [None]*num_output

    for i in range(num_output):
        pred_node_names[i] = output_node_names_of_final_network+str(i)
        pred[i] = tf.identity(net_model.output[i], name=pred_node_names[i])

    sess = K.get_session()

    constant_graph = graph_util.convert_variables_to_constants(
        sess, sess.graph.as_graph_def(), pred_node_names)
    graph_io.write_graph(
        constant_graph, output_fld, output_graph_name, as_text=False)
    print('saved the constant graph (ready for inference) at: ',
         osp.join(output_fld, output_graph_name))

    return output_fld+output_graph_name


# tf_model_path = convert_to_pb(
# 'generator_weights.hdf5','generator.json','saved_model/','tf_model/')
\end{lstlisting}
% subsection converting_models (end)
\par\bigskip

\subsection{Semantic Inpainting} % (fold)
\label{sub:semantic_inpainting}
This code implements semantic inpainting for demonstration purposes.
\begin{lstlisting}[basicstyle=\scriptsize,language=Python]
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import cv2

IMAGE_SHAPE=[28,28,1]
ITERATIONS=1000

gen_input_name="input_2:0"
gen_output_name="output_node0:0"
dis_input_name="input_1:0"
dis_output_name="output_node0:0"

momentum,lr=0.9,0.01


# read  graph definition from protocol buffer
gen_graph_def = tf.GraphDef()
with open('generator.pb', "rb") as f:
    gen_graph_def.ParseFromString(f.read())


z=tf.placeholder(tf.float32,shape=(1,100))
gen_imgs,=tf.import_graph_def(gen_graph_def,input_map={gen_input_name:z},
     return_elements=[gen_output_name])
print("loaded generator")

dis_graph_def = tf.GraphDef()
with open('discriminator.pb', "rb") as f:
    dis_graph_def.ParseFromString(f.read())
dis_imgs=tf.reshape(gen_imgs,shape=[1]+IMAGE_SHAPE)
print(dis_imgs.shape)


valid,=tf.import_graph_def(dis_graph_def,input_map={dis_input_name:dis_imgs},
     return_elements=[dis_output_name])
print("loaded discriminator")

mask = tf.placeholder(tf.float32, IMAGE_SHAPE, name='mask')
image = tf.placeholder(tf.float32, IMAGE_SHAPE, name='image')
contextual_loss = tf.reduce_sum(
    tf.contrib.layers.flatten(
        tf.abs(tf.multiply(mask, gen_imgs) - tf.multiply(mask, image))), 1)
perceptual_loss = tf.log(1-valid)
complete_loss = contextual_loss + 0.1*perceptual_loss
grad_complete_loss = tf.gradients(complete_loss, z)



with tf.Session() as sess:
    scale = 0.3
    mask_val=np.ones(IMAGE_SHAPE)
    l = int(28*scale)
    u = int(28*(1.0-scale))
    mask_val[l:u, l:u, :] = 0.0


    in_image=(cv2.imread('img_269.jpg',0))

    # expand dimension if its gray scale image
    if IMAGE_SHAPE[2]==1:
        in_image=in_image.reshape(IMAGE_SHAPE)

    in_image=(in_image.astype(np.float32)-127.5)/127.5

    masked_image=(1+np.multiply(in_image,mask_val))/2

    if IMAGE_SHAPE[2]==1:
        masked_image=masked_image[:,:,0]

    cv2.imwrite('front/image.png',masked_image*255)

    zhats=np.random.normal(0, 1, (1, 100))
    v=0
    for i in range(ITERATIONS):
        fd={
        mask:mask_val,
        image:in_image,
        z:zhats
        }
        outputs=[complete_loss, grad_complete_loss, gen_imgs]
        loss,grad,g_imgs=sess.run(outputs,feed_dict=fd)
        v_prev = np.copy(v)
        v = momentum*v - lr*grad[0]
        zhats += -momentum * v_prev + (1+momentum)*v
        zhats = np.clip(zhats, 0, 1)
        if(i%10==0):
            print("Iteration {}".format(i))

        # write current status  
        completed_img=np.multiply(g_imgs,1-mask_val)+
            np.multiply(in_image,mask_val)
        completed_img=(1+completed_img)*127.5
        
        if IMAGE_SHAPE[2]==1:
            completed_img=completed_img[:,:,0]

        cv2.imwrite('front/image.png',completed_img)

    g_imgs=sess.run(gen_imgs,feed_dict={z:zhats})

    completed_img=np.multiply(g_imgs,1-mask_val)+
        np.multiply(in_image,mask_val)
    completed_img=(1+completed_img)/2

    original_image=(1+in_image)/2

    cmap=None

    if IMAGE_SHAPE[2]==1:
        completed_img=completed_img[:,:,0]
        original_image=original_image[:,:,0]            
        cmap='gray'


    fig, axs = plt.subplots(1, 3)
    axs[0].set_title("Original")
    axs[0].imshow(original_image,cmap=cmap)
    axs[0].axis('off')
    axs[1].set_title("Masked")
    axs[1].imshow(masked_image,cmap=cmap)
    axs[1].axis('off')
    axs[2].set_title("Completed")
    axs[2].imshow(completed_img,cmap=cmap)
    axs[2].axis('off')
    plt.show()

\end{lstlisting}
% subsection semantic_inpainting (end)
\par\bigskip

% section demonstration (end)