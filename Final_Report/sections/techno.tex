\chapter{Technology}\label{ch:technology}
\epigraph{\textit{\normalsize“I think, therefore I am”}}{\textit{ \normalsize René Descartes,\\ French philosopher and scientist}}
Deep learning frameworks offer flexibility with designing and training custom deep neural networks and provide interfaces to common programming language. We used the following frameworks and technologies in our project.

\section{Tensorflow} % (fold)
\label{sec:tensorflow}
TensorFlow is an open source software library for high performance numerical computation. Its flexible architecture allows easy deployment of computation across a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters of servers to mobile and edge devices. Originally developed by researchers and engineers from the Google Brain team within Google’s AI organization, it comes with strong support for machine learning and deep learning and the flexible numerical computation core is used across many other scientific domains. TensorFlow, as the name indicates, is a framework to define and run computations involving tensors. A tensor is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes. TensorFlow uses a dataflow graph to represent your computation in terms of the dependencies between individual operations. This leads to a low-level programming model in which you first define the dataflow graph, then create a TensorFlow session to run parts of the graph across a set of local and remote devices.\par\bigskip
We use Tensorflow when we need access to low level API such as metric functions or auto gradient functions.
% section tensorflow (end)

\section{Keras} % (fold)
\label{sec:keras}
Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. It puts user experience front and center. Keras follows best practices for reducing cognitive load: it offers consistent \& simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear and actionable feedback upon user error. A model is understood as a sequence or a graph of standalone, fully-configurable modules that can be plugged together with as little restrictions as possible. In particular, neural layers, cost functions, optimizers, initialization schemes, activation functions, regularization schemes are all standalone modules that you can combine to create new models. Can easily create new modules allows for total expressiveness, making Keras suitable for advanced research.\par\bigskip
We use Keras as our primary Deep learning library. Most of our code uses keras. 
% section keras (end)

\section{PyTorch} % (fold)
\label{sec:pytorch}
PyTorch is a relatively new framework. PyTorch provides Tensors that can live either on the CPU or the GPU, and accelerate compute by a huge amount. It also provides a wide variety of tensor routines to accelerate and fit your scientific computation needs such as slicing, indexing, math operations, linear algebra, reductions. PyTorch has a unique way of building neural networks: using and replaying a tape recorder. Most frameworks such as TensorFlow, Theano, Caffe and CNTK have a static view of the world. One has to build a neural network, and reuse the same structure again and again. Changing the way the network behaves means that one has to start from scratch. With PyTorch, it uses a technique called Reverse-mode auto-differentiation, which allows you to change the way your network behaves arbitrarily with zero lag or overhead. Its inspiration comes from several research papers on this topic, as well as current and past work such as autograd, autograd, Chainer, etc. While this technique is not unique to PyTorch, it’s one of the fastest implementations of it to date. \par\bigskip
We have a simple proof of concept implementation of our project in PyTorch, which can easily be extended to other GANs.
% section pytorch (end)

\section{Google Colaboratory} % (fold)
\label{sec:google_colaboratory}
Colaboratory is a research tool for machine learning education and research. Colaboratory is a Google research project created to help disseminate machine learning education and research. It’s a Jupyter notebook environment that requires no setup to use and runs entirely in the cloud. We can use GPU as a backend for free for 12 hours at a time. The GPU used in the backend is Nvidia Tesla K80. Colaboratory notebooks are stored in Google Drive and can be shared just as you would with Google Docs or Sheets. Colaboratory supports both Python2 and Python3 for code execution. It has Intel Xeon 2vCPU running at 2.2 GHz, 13 GB RAM and 33 GB storage space.
We used Google Colaboratory extensively for our project. We trained and tested all of our models on colaboratory. It provides an average 10 times speed-up than running on a local machine. 
% section google_colaboratory (end)


% \begin{itemize}
% 	\item\textbf{Adversarial Training:} Two models undergoing training simultaneously by competing against each other. The output of each model acts as an adversary for the other to improve upon.
% 	\item\textbf{Generative Model:} Any model capable of generating completely new realistic data of a class.
% 	\item\textbf{Discriminative Model:} Here, as a Discriminator, a model which is capable of distinguishing between actual ground truth from the true distribution and generated information from a model.
% 	\item\textbf{Gradient Descent:} A method of optimizing an objective function using a first-order iterative approach to find a local minimum.
% 	\item\textbf{Gaussian Model:} A model that fits the data in the shape of a Gaussian function, identified by a characteristic "bell curve". CapsNet uses this to find agreeing probability outputs of the "capsules".
% 	\item\textbf{TensorFlow:} An open source software library for numerical computation using data flow graphs. It was originally developed by the Google Brain Team within Google's Machine Intelligence research organization for machine learning and deep neural networks research.

% \end{itemize}

% \textbf{\Large Other essentials:}

% \begin{itemize}
% 	\item\textbf{CPU:} 3GHz quad core, x86-64 architecture (or)
% 	\item\textbf{GPU:} NVIDIA or any other TensorFlow supported GPU, CUDA or cuDNN
% 	\item\textbf{Python:} 2.7 or 3.4 and above
% \end{itemize}
